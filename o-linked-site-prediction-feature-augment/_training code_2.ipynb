{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# function ConnectButton(){\n",
    "#     console.log(\"Connect pushed\");\n",
    "#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
    "# }\n",
    "\n",
    "# setInterval(ConnectButton,60000);\n",
    "# '''\n",
    "\n",
    "# from google.colab import drive\n",
    "# from os import chdir\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# project_path = '/content/drive/MyDrive/Gproject/o-linked-site-prediction-feature-augment'\n",
    "# chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set seed for the reproducible result\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of initial proteins: 104\n",
      "['A2ABU4', 'A2AHJ4', 'A2AKB9', 'A2AQ25', 'E9Q1P8', 'E9Q5G3', 'O08537', 'O09061', 'O35303', 'O70263']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/integrated_features' # we will get names from the augmented proteins\n",
    "protein_names = [x.split('.')[0] for x in os.listdir(data_dir) if x.split('.')[1] == 'csv'] # get protein name list to be processed for building machine learning models\n",
    "print('the number of initial proteins:', len(protein_names))\n",
    "print(protein_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameter optimization by K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ml_models import *\n",
    "\n",
    "epochs = 1000\n",
    "from keras.callbacks import EarlyStopping\n",
    "patience = 30\n",
    "callbacks = [EarlyStopping(patience=patience, restore_best_weights=True, monitor='val_loss')]\n",
    "\n",
    "test_size = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters\n",
      "rnn_layers     : 1\n",
      "rnn_neurons    : 64\n",
      "dnn_layers     : 3\n",
      "dnn_neurons    : 64\n",
      "activation     : softmax\n",
      "loss           : categorical_crossentropy\n",
      "metrics        : accuracy\n",
      "optimizer_type : Adam\n",
      "learning_rate  : 0.001\n",
      "regularizer    : {'input': None, 'hidden': None, 'bias': None}\n",
      "window_size    : 10\n"
     ]
    }
   ],
   "source": [
    "initial_params = default_params.copy()\n",
    "initial_params.update({\n",
    "    'window_size'    : 10\n",
    "    })\n",
    "\n",
    "print('initial parameters')\n",
    "for key, value in initial_params.items():\n",
    "    print(f'{key:<14} : {value}')\n",
    "\n",
    "search_space = {\n",
    "    'dnn_layers'    : [1, 2, 3, 4, 5],\n",
    "    'dnn_neurons'   : [32, 64, 128, 256],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuous features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features:\n",
      "{0: 'residue'}\n"
     ]
    }
   ],
   "source": [
    "model_type = 'MLP'\n",
    "\n",
    "# set continuous input features\n",
    "x_cts = []\n",
    "print('continuous features:')\n",
    "display(dict(zip(range(len(x_cts)), x_cts)))\n",
    "\n",
    "# set categorical input features\n",
    "x_cat = ['residue']\n",
    "print('categorical features:')\n",
    "print(dict(zip(range(len(x_cat)), x_cat)))\n",
    "\n",
    "# input features\n",
    "x_var = x_cts + x_cat\n",
    "\n",
    "# set continuos output feature\n",
    "y_cts = []\n",
    "\n",
    "# set categorical output feature\n",
    "y_cat = ['positivity']\n",
    "\n",
    "# output features\n",
    "y_var = y_cts + y_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build amino acid sequence dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training with K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>regularizer_bias</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.158818</td>\n",
       "      <td>96.278</td>\n",
       "      <td>96.278</td>\n",
       "      <td>20.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>98.104</td>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2486</td>\n",
       "      <td>0.160254</td>\n",
       "      <td>96.270</td>\n",
       "      <td>96.278</td>\n",
       "      <td>10.000</td>\n",
       "      <td>99.990</td>\n",
       "      <td>0.232</td>\n",
       "      <td>98.100</td>\n",
       "      <td>0.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.165711</td>\n",
       "      <td>96.252</td>\n",
       "      <td>96.278</td>\n",
       "      <td>5.000</td>\n",
       "      <td>99.972</td>\n",
       "      <td>0.232</td>\n",
       "      <td>98.090</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2232</td>\n",
       "      <td>0.162925</td>\n",
       "      <td>96.270</td>\n",
       "      <td>96.286</td>\n",
       "      <td>20.000</td>\n",
       "      <td>99.982</td>\n",
       "      <td>0.466</td>\n",
       "      <td>98.098</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.160589</td>\n",
       "      <td>96.262</td>\n",
       "      <td>96.270</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.990</td>\n",
       "      <td>0.000</td>\n",
       "      <td>98.096</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1994</td>\n",
       "      <td>0.160905</td>\n",
       "      <td>96.278</td>\n",
       "      <td>96.278</td>\n",
       "      <td>20.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>98.104</td>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2232</td>\n",
       "      <td>0.162925</td>\n",
       "      <td>96.270</td>\n",
       "      <td>96.286</td>\n",
       "      <td>20.000</td>\n",
       "      <td>99.982</td>\n",
       "      <td>0.466</td>\n",
       "      <td>98.098</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.167437</td>\n",
       "      <td>96.260</td>\n",
       "      <td>96.294</td>\n",
       "      <td>46.666</td>\n",
       "      <td>99.964</td>\n",
       "      <td>0.696</td>\n",
       "      <td>98.094</td>\n",
       "      <td>1.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.174028</td>\n",
       "      <td>96.176</td>\n",
       "      <td>96.290</td>\n",
       "      <td>21.666</td>\n",
       "      <td>99.870</td>\n",
       "      <td>0.696</td>\n",
       "      <td>98.050</td>\n",
       "      <td>1.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3406</td>\n",
       "      <td>0.161321</td>\n",
       "      <td>96.278</td>\n",
       "      <td>96.278</td>\n",
       "      <td>20.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>98.104</td>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.167437</td>\n",
       "      <td>96.260</td>\n",
       "      <td>96.294</td>\n",
       "      <td>46.666</td>\n",
       "      <td>99.964</td>\n",
       "      <td>0.696</td>\n",
       "      <td>98.094</td>\n",
       "      <td>1.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3092</td>\n",
       "      <td>0.160127</td>\n",
       "      <td>96.252</td>\n",
       "      <td>96.270</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.982</td>\n",
       "      <td>0.000</td>\n",
       "      <td>98.090</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                             \n",
       "1            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "2            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "3            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "4            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "5            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "6            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "7            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "8            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "9            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "10           2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "11           2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "12           2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  ...  \\\n",
       "model_id                                                       ...   \n",
       "1                64.0         1.0         64.0         0.0010  ...   \n",
       "2                64.0         2.0         64.0         0.0010  ...   \n",
       "3                64.0         3.0         64.0         0.0010  ...   \n",
       "4                64.0         4.0         64.0         0.0010  ...   \n",
       "5                64.0         5.0         64.0         0.0010  ...   \n",
       "6                64.0         4.0         32.0         0.0010  ...   \n",
       "7                64.0         4.0         64.0         0.0010  ...   \n",
       "8                64.0         4.0        128.0         0.0010  ...   \n",
       "9                64.0         4.0        256.0         0.0010  ...   \n",
       "10               64.0         4.0        128.0         0.0001  ...   \n",
       "11               64.0         4.0        128.0         0.0010  ...   \n",
       "12               64.0         4.0        128.0         0.0100  ...   \n",
       "\n",
       "          regularizer_bias  training_time  test_loss  accuracy  precision_0  \\\n",
       "model_id                                                                      \n",
       "1                      NaN         0.2278   0.158818    96.278       96.278   \n",
       "2                      NaN         0.2486   0.160254    96.270       96.278   \n",
       "3                      NaN         0.2218   0.165711    96.252       96.278   \n",
       "4                      NaN         0.2232   0.162925    96.270       96.286   \n",
       "5                      NaN         0.2372   0.160589    96.262       96.270   \n",
       "6                      NaN         0.1994   0.160905    96.278       96.278   \n",
       "7                      NaN         0.2232   0.162925    96.270       96.286   \n",
       "8                      NaN         0.3070   0.167437    96.260       96.294   \n",
       "9                      NaN         0.4298   0.174028    96.176       96.290   \n",
       "10                     NaN         0.3406   0.161321    96.278       96.278   \n",
       "11                     NaN         0.3070   0.167437    96.260       96.294   \n",
       "12                     NaN         0.3092   0.160127    96.252       96.270   \n",
       "\n",
       "          precision_1  recall_0  recall_1    f1_0   f1_1  \n",
       "model_id                                                  \n",
       "1              20.000   100.000     0.232  98.104  0.460  \n",
       "2              10.000    99.990     0.232  98.100  0.454  \n",
       "3               5.000    99.972     0.232  98.090  0.444  \n",
       "4              20.000    99.982     0.466  98.098  0.910  \n",
       "5               0.000    99.990     0.000  98.096  0.000  \n",
       "6              20.000   100.000     0.232  98.104  0.460  \n",
       "7              20.000    99.982     0.466  98.098  0.910  \n",
       "8              46.666    99.964     0.696  98.094  1.370  \n",
       "9              21.666    99.870     0.696  98.050  1.316  \n",
       "10             20.000   100.000     0.232  98.104  0.460  \n",
       "11             46.666    99.964     0.696  98.094  1.370  \n",
       "12              0.000    99.982     0.000  98.090  0.000  \n",
       "\n",
       "[12 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn_layers     : [1, 2, 3, '4', 5]\n",
      "dnn_neurons    : [32, 64, '128', 256]\n",
      "learning_rate  : [0.0001, '0.001', 0.01]\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "model_update  = False\n",
    "\n",
    "params = initial_params.copy()\n",
    "MODELs = []\n",
    "METRICs = []\n",
    "METRIC_MEAN = []\n",
    "model_id = 1\n",
    "verbose = 0\n",
    "for param_name, space in search_space.items():\n",
    "    for point in space:\n",
    "        clear_output(wait=True)\n",
    "        display(METRIC_MEAN)\n",
    "        params[param_name] = point\n",
    "        for key, value in search_space.items():\n",
    "            print(f'{key:<14} : {value}')\n",
    "        \n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for name in protein_names:\n",
    "            data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "            ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "            \n",
    "            # get X dataset\n",
    "            x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "            x_features = list(x_onehot.columns)\n",
    "            \n",
    "            # get Y dataset\n",
    "            y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "            y_labels = list(y_onehot.columns)\n",
    "            \n",
    "            for idx in ST_idx:\n",
    "                window_x = np.array(get_window(x_onehot, idx, params['window_size']))\n",
    "                window_x = window_x.reshape(-1)\n",
    "                label_y  = np.array(y_onehot.iloc[idx])\n",
    "                \n",
    "                data_x.append(window_x)\n",
    "                data_y.append(label_y)\n",
    "                \n",
    "        data_x = np.array(data_x)\n",
    "        data_y = np.array(data_y)\n",
    "        model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params)  # I don't know why, but this row is helping producing the same training result of a neural network\n",
    "\n",
    "        print('data x shape:', data_x.shape)\n",
    "        print('data y shape:', data_y.shape)\n",
    "        print('class y counts:', data_y.sum(0))\n",
    "        print(f'class y ratio: {(data_y.sum(0)/len(data_y)).round(4)}')\n",
    "        \n",
    "        splitter = StratifiedShuffleSplit(n_splits = 1, test_size = test_size, random_state = SEED)\n",
    "        train_idx, test_idx = list(splitter.split(data_x, data_y))[0]\n",
    "        \n",
    "        train_x = data_x[train_idx]\n",
    "        train_y = data_y[train_idx]\n",
    "        \n",
    "        test_x = data_x[test_idx]\n",
    "        test_y = data_y[test_idx]\n",
    "        \n",
    "        train_x, test_x = data_scaling(train_x, test_x)\n",
    "        \n",
    "        splitter_kf = KFold(n_splits = 5)\n",
    "        for cv_idx, (train_idx_kf, test_idx_kf) in enumerate(splitter_kf.split(train_x, train_y)):\n",
    "            train_x_kf, train_y_kf = train_x[train_idx_kf], train_y[train_idx_kf]\n",
    "            test_x_kf, test_y_kf   = train_x[test_idx_kf],  train_y[test_idx_kf]\n",
    "            \n",
    "            model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params) \n",
    "            model_name = name_model(f'{model_type}_KFOLD', params)\n",
    "            \n",
    "            model_folder  = f'./models/{model_name}_{train_x.shape}_{test_x.shape}'\n",
    "            if not os.path.exists(model_folder):\n",
    "                os.makedirs(model_folder)\n",
    "            model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "            metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "            \n",
    "            \n",
    "            if not os.path.exists(model_path) or model_update:\n",
    "                time_start = time.time()\n",
    "                history = model.fit(train_x_kf, train_y_kf, verbose=verbose, \n",
    "                                    epochs = 10000, callbacks = callbacks,\n",
    "                                    validation_data = (test_x_kf, test_y_kf))\n",
    "                time_end = time.time()\n",
    "                training_time = round((time_end - time_start)/60, 3)\n",
    "                \n",
    "                model.save_weights(model_path)\n",
    "                \n",
    "                test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "                model_metrics = {\n",
    "                    'model_id' : model_id,\n",
    "                    'cv_idx'   : cv_idx,\n",
    "                    **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "                    'train_y'     : train_y.shape[-1],\n",
    "                    'test_size'   : test_x.shape[0],\n",
    "                    **params,\n",
    "                    'regularizer_input' : params['regularizer']['input'],\n",
    "                    'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "                    'regularizer_bias' : params['regularizer']['bias'],\n",
    "                    'training_time': training_time,\n",
    "                    'test_loss': test_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "                    **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "                    **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "                \n",
    "                model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "                model_metrics.to_csv(metric_path, index=False)\n",
    "                \n",
    "            else:\n",
    "                model.load_weights(model_path)\n",
    "                model_metrics = pd.read_csv(metric_path, header=0)\n",
    "                \n",
    "            print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "            \n",
    "            model_metrics['model_id'] = model_id\n",
    "            METRICs.append(model_metrics)\n",
    "            MODELs.append(model)\n",
    "        \n",
    "        METRIC_MEAN = pd.concat(METRICs).groupby('model_id').mean()\n",
    "        f1_best = METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0].f1_1\n",
    "        print(f'best f1 score: {f1_best}')\n",
    "        model_id += 1\n",
    "    if param_name in ['learning_rate']: # for float-type parameters\n",
    "        best_value = float(METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0][param_name])\n",
    "        params[param_name] = best_value\n",
    "        search_space[param_name][search_space[param_name].index(best_value)] = f\"{best_value}\"\n",
    "    else: # for int-type parameters\n",
    "        best_value = int(METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0][param_name])\n",
    "        params[param_name] = best_value\n",
    "        search_space[param_name][search_space[param_name].index(best_value)] = f\"{best_value}\"\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(METRIC_MEAN)\n",
    "for key, value in search_space.items():\n",
    "    print(f'{key:<14} : {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape:  (11528, 420)\n",
      "data y shape:  (11528, 2)\n",
      "train x shape: (9222, 420)\n",
      "test  x shape: (2306, 2)\n"
     ]
    }
   ],
   "source": [
    "print('data x shape: ', data_x.shape)\n",
    "print('data y shape: ', data_y.shape)\n",
    "print('train x shape:', train_x.shape)\n",
    "print('test  x shape:', test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal parameters\n",
      "rnn_layers     : 1\n",
      "rnn_neurons    : 64\n",
      "dnn_layers     : 4\n",
      "dnn_neurons    : 128\n",
      "activation     : softmax\n",
      "loss           : categorical_crossentropy\n",
      "metrics        : accuracy\n",
      "optimizer_type : Adam\n",
      "learning_rate  : 0.001\n",
      "regularizer    : {'input': None, 'hidden': None, 'bias': None}\n",
      "window_size    : 10\n"
     ]
    }
   ],
   "source": [
    "print('optimal parameters')\n",
    "for key, value in params.items():\n",
    "    print(f'{key:<14} : {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model's general performance through Monte-Carlo cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape: (11528, 420)\n",
      "data y shape: (11528, 2)\n",
      "class y counts: [11100   428]\n",
      "class y ratio: [0.9629 0.0371]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n",
      "c:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.0\n",
      "f1 score: 0.0\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "model_update = False\n",
    "            \n",
    "MODELs = []\n",
    "METRICs = []\n",
    "METRIC_MEAN = []\n",
    "verbose = 0\n",
    "        \n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for name in protein_names:\n",
    "    data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "    ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "    \n",
    "    # get X dataset\n",
    "    x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "    x_features = list(x_onehot.columns)\n",
    "    \n",
    "    # get Y dataset\n",
    "    y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "    y_labels = list(y_onehot.columns)\n",
    "    \n",
    "    for idx in ST_idx:\n",
    "        window_x = np.array(get_window(x_onehot, idx, params['window_size']))\n",
    "        window_x = window_x.reshape(-1)\n",
    "        label_y  = np.array(y_onehot.iloc[idx])\n",
    "        \n",
    "        data_x.append(window_x)\n",
    "        data_y.append(label_y)\n",
    "        \n",
    "data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "\n",
    "print('data x shape:', data_x.shape)\n",
    "print('data y shape:', data_y.shape)\n",
    "print('class y counts:', data_y.sum(0))\n",
    "print(f'class y ratio: {(data_y.sum(0)/len(data_y)).round(4)}')\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits = 5, test_size = test_size, random_state = SEED)\n",
    "for cv_idx, (train_idx, test_idx) in enumerate(splitter.split(data_x, data_y)):\n",
    "    train_x, train_y = data_x[train_idx], data_y[train_idx]\n",
    "    test_x , test_y  = data_x[test_idx],  data_y[test_idx]\n",
    "    \n",
    "    train_x, test_x = data_scaling(train_x, test_x)\n",
    "    \n",
    "    model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params)\n",
    "    model_name = name_model(f'{model_type}', params)\n",
    "    \n",
    "    \n",
    "    model_folder  = f'./models/{model_name}_{train_x.shape}_{test_x.shape}'\n",
    "    if not os.path.exists(model_folder):\n",
    "        os.makedirs(model_folder)\n",
    "    model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "    metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(model_path) or model_update:\n",
    "        time_start = time.time()\n",
    "        history = model.fit(train_x, train_y, verbose=verbose, \n",
    "                            epochs = 10000, callbacks = callbacks,\n",
    "                            validation_split = test_size/(1-test_size))\n",
    "        time_end = time.time()\n",
    "        training_time = round((time_end - time_start)/60, 3)\n",
    "        \n",
    "        model.save_weights(model_path)\n",
    "        \n",
    "        test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "        model_metrics = {\n",
    "            'model_id' : model_id,\n",
    "            'cv_idx'   : cv_idx,\n",
    "            **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "            'train_y'     : train_y.shape[-1],\n",
    "            'test_size'   : test_x.shape[0],\n",
    "            **params,\n",
    "            'regularizer_input' : params['regularizer']['input'],\n",
    "            'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "            'regularizer_bias' : params['regularizer']['bias'],\n",
    "            'training_time': training_time,\n",
    "            'test_loss': test_loss,\n",
    "            'accuracy': accuracy,\n",
    "            **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "            **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "            **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "        model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "        model_metrics.to_csv(metric_path, index=False)\n",
    "        \n",
    "    else:\n",
    "        model.load_weights(model_path)\n",
    "        model_metrics = pd.read_csv(metric_path, header=0)\n",
    "        \n",
    "    print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "    \n",
    "    METRICs.append(model_metrics)\n",
    "    MODELs.append(model)\n",
    "\n",
    "METRICs = pd.concat(METRICs)\n",
    "METRIC_MEAN = METRICs.groupby('model_id').mean()\n",
    "METRIC_STD = METRICs.groupby('model_id').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>...</th>\n",
       "      <th>regularizer_bias</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>9222</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.160427</td>\n",
       "      <td>96.27</td>\n",
       "      <td>96.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>9222</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.159890</td>\n",
       "      <td>96.27</td>\n",
       "      <td>96.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9222</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.162230</td>\n",
       "      <td>96.27</td>\n",
       "      <td>96.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9222</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.148290</td>\n",
       "      <td>96.27</td>\n",
       "      <td>96.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>9222</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.160771</td>\n",
       "      <td>96.05</td>\n",
       "      <td>96.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "0        13       0     9222      420        2       2306           1   \n",
       "0        13       1     9222      420        2       2306           1   \n",
       "0        13       2     9222      420        2       2306           1   \n",
       "0        13       3     9222      420        2       2306           1   \n",
       "0        13       4     9222      420        2       2306           1   \n",
       "\n",
       "   rnn_neurons  dnn_layers  dnn_neurons  ...  regularizer_bias  training_time  \\\n",
       "0           64           4          128  ...              None          0.341   \n",
       "0           64           4          128  ...              None          0.347   \n",
       "0           64           4          128  ...              None          0.316   \n",
       "0           64           4          128  ...              None          0.324   \n",
       "0           64           4          128  ...              None          0.327   \n",
       "\n",
       "  test_loss accuracy precision_0  precision_1  recall_0  recall_1   f1_0  f1_1  \n",
       "0  0.160427    96.27       96.27          0.0    100.00       0.0  98.10   0.0  \n",
       "0  0.159890    96.27       96.27          0.0    100.00       0.0  98.10   0.0  \n",
       "0  0.162230    96.27       96.27          0.0    100.00       0.0  98.10   0.0  \n",
       "0  0.148290    96.27       96.27          0.0    100.00       0.0  98.10   0.0  \n",
       "0  0.160771    96.05       96.26          0.0     99.77       0.0  97.99   0.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRICs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>window_size</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.158322</td>\n",
       "      <td>96.226</td>\n",
       "      <td>96.268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.078</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                             \n",
       "13           2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  window_size  \\\n",
       "model_id                                                                     \n",
       "13               64.0         4.0        128.0          0.001         10.0   \n",
       "\n",
       "          training_time  test_loss  accuracy  precision_0  precision_1  \\\n",
       "model_id                                                                 \n",
       "13                0.331   0.158322    96.226       96.268          0.0   \n",
       "\n",
       "          recall_0  recall_1    f1_0  f1_1  \n",
       "model_id                                    \n",
       "13          99.954       0.0  98.078   0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRIC_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>window_size</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>0.098387</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049193</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                               \n",
       "13        1.581139      0.0      0.0      0.0        0.0         0.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  window_size  \\\n",
       "model_id                                                                     \n",
       "13                0.0         0.0          0.0            0.0          0.0   \n",
       "\n",
       "          training_time  test_loss  accuracy  precision_0  precision_1  \\\n",
       "model_id                                                                 \n",
       "13             0.012708   0.005674  0.098387     0.004472          0.0   \n",
       "\n",
       "          recall_0  recall_1      f1_0  f1_1  \n",
       "model_id                                      \n",
       "13        0.102859       0.0  0.049193   0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRIC_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# function ConnectButton(){\n",
    "#     console.log(\"Connect pushed\");\n",
    "#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
    "# }\n",
    "\n",
    "# setInterval(ConnectButton,60000);\n",
    "# '''\n",
    "\n",
    "# from google.colab import drive\n",
    "# from os import chdir\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# project_path = '/content/drive/MyDrive/Gproject/o-linked-site-prediction-feature-augment'\n",
    "# chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set seed for the reproducible result\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of initial proteins: 104\n",
      "['A2ABU4', 'A2AHJ4', 'A2AKB9', 'A2AQ25', 'E9Q1P8', 'E9Q5G3', 'O08537', 'O09061', 'O35303', 'O70263']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/integrated_features' # we will get names from the augmented proteins\n",
    "protein_names = [x.split('.')[0] for x in os.listdir(data_dir) if x.split('.')[1] == 'csv'] # get protein name list to be processed for building machine learning models\n",
    "print('the number of initial proteins:', len(protein_names))\n",
    "print(protein_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameter optimization by K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ml_models import *\n",
    "\n",
    "epochs = 1000\n",
    "from keras.callbacks import EarlyStopping\n",
    "patience = 30\n",
    "callbacks = [EarlyStopping(patience=patience, restore_best_weights=True, monitor='val_loss')]\n",
    "\n",
    "test_size = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters\n",
      "rnn_layers     : 1\n",
      "rnn_neurons    : 64\n",
      "dnn_layers     : 3\n",
      "dnn_neurons    : 64\n",
      "activation     : softmax\n",
      "loss           : categorical_crossentropy\n",
      "metrics        : accuracy\n",
      "optimizer_type : Adam\n",
      "learning_rate  : 0.001\n",
      "regularizer    : {'input': None, 'hidden': None, 'bias': None}\n",
      "window_size    : 10\n"
     ]
    }
   ],
   "source": [
    "initial_params = default_params.copy()\n",
    "initial_params.update({\n",
    "    'window_size'    : 10\n",
    "    })\n",
    "\n",
    "print('initial parameters')\n",
    "for key, value in initial_params.items():\n",
    "    print(f'{key:<14} : {value}')\n",
    "\n",
    "search_space = {\n",
    "    'dnn_layers'    : [1, 2, 3, 4, 5],\n",
    "    'dnn_neurons'   : [32, 64, 128, 256],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuous features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features:\n",
      "{0: 'residue'}\n"
     ]
    }
   ],
   "source": [
    "model_type = 'MLP_UP'\n",
    "\n",
    "# set continuous input features\n",
    "x_cts = []\n",
    "print('continuous features:')\n",
    "display(dict(zip(range(len(x_cts)), x_cts)))\n",
    "\n",
    "# set categorical input features\n",
    "x_cat = ['residue']\n",
    "print('categorical features:')\n",
    "print(dict(zip(range(len(x_cat)), x_cat)))\n",
    "\n",
    "# input features\n",
    "x_var = x_cts + x_cat\n",
    "\n",
    "# set continuos output feature\n",
    "y_cts = []\n",
    "\n",
    "# set categorical output feature\n",
    "y_cat = ['positivity']\n",
    "\n",
    "# output features\n",
    "y_var = y_cts + y_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build amino acid sequence dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training with K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>regularizer_bias</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3920</td>\n",
       "      <td>0.448702</td>\n",
       "      <td>80.418</td>\n",
       "      <td>96.942</td>\n",
       "      <td>6.764</td>\n",
       "      <td>82.252</td>\n",
       "      <td>33.024</td>\n",
       "      <td>88.988</td>\n",
       "      <td>11.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>0.447681</td>\n",
       "      <td>80.746</td>\n",
       "      <td>96.896</td>\n",
       "      <td>6.620</td>\n",
       "      <td>82.648</td>\n",
       "      <td>31.628</td>\n",
       "      <td>89.198</td>\n",
       "      <td>10.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.448122</td>\n",
       "      <td>80.392</td>\n",
       "      <td>96.890</td>\n",
       "      <td>6.524</td>\n",
       "      <td>82.272</td>\n",
       "      <td>31.860</td>\n",
       "      <td>88.976</td>\n",
       "      <td>10.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6952</td>\n",
       "      <td>0.445132</td>\n",
       "      <td>80.244</td>\n",
       "      <td>96.918</td>\n",
       "      <td>6.592</td>\n",
       "      <td>82.090</td>\n",
       "      <td>32.556</td>\n",
       "      <td>88.884</td>\n",
       "      <td>10.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6646</td>\n",
       "      <td>0.441334</td>\n",
       "      <td>80.548</td>\n",
       "      <td>96.916</td>\n",
       "      <td>6.684</td>\n",
       "      <td>82.414</td>\n",
       "      <td>32.326</td>\n",
       "      <td>89.068</td>\n",
       "      <td>11.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3496</td>\n",
       "      <td>0.445958</td>\n",
       "      <td>80.330</td>\n",
       "      <td>97.000</td>\n",
       "      <td>6.936</td>\n",
       "      <td>82.108</td>\n",
       "      <td>34.418</td>\n",
       "      <td>88.930</td>\n",
       "      <td>11.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3920</td>\n",
       "      <td>0.448702</td>\n",
       "      <td>80.418</td>\n",
       "      <td>96.942</td>\n",
       "      <td>6.764</td>\n",
       "      <td>82.252</td>\n",
       "      <td>33.024</td>\n",
       "      <td>88.988</td>\n",
       "      <td>11.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>0.450252</td>\n",
       "      <td>80.858</td>\n",
       "      <td>96.870</td>\n",
       "      <td>6.540</td>\n",
       "      <td>82.792</td>\n",
       "      <td>30.928</td>\n",
       "      <td>89.274</td>\n",
       "      <td>10.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7780</td>\n",
       "      <td>0.449549</td>\n",
       "      <td>80.918</td>\n",
       "      <td>96.862</td>\n",
       "      <td>6.506</td>\n",
       "      <td>82.866</td>\n",
       "      <td>30.696</td>\n",
       "      <td>89.312</td>\n",
       "      <td>10.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4128</td>\n",
       "      <td>0.474191</td>\n",
       "      <td>77.822</td>\n",
       "      <td>97.070</td>\n",
       "      <td>6.692</td>\n",
       "      <td>79.360</td>\n",
       "      <td>38.140</td>\n",
       "      <td>87.324</td>\n",
       "      <td>11.382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3496</td>\n",
       "      <td>0.445958</td>\n",
       "      <td>80.330</td>\n",
       "      <td>97.000</td>\n",
       "      <td>6.936</td>\n",
       "      <td>82.108</td>\n",
       "      <td>34.418</td>\n",
       "      <td>88.930</td>\n",
       "      <td>11.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3934</td>\n",
       "      <td>0.447344</td>\n",
       "      <td>80.232</td>\n",
       "      <td>96.886</td>\n",
       "      <td>6.462</td>\n",
       "      <td>82.108</td>\n",
       "      <td>31.862</td>\n",
       "      <td>88.880</td>\n",
       "      <td>10.734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                             \n",
       "1            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "2            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "3            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "4            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "5            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "6            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "7            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "8            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "9            2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "10           2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "11           2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "12           2.0   9222.0    420.0      2.0     2306.0         1.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  ...  \\\n",
       "model_id                                                       ...   \n",
       "1                64.0         1.0         64.0         0.0010  ...   \n",
       "2                64.0         2.0         64.0         0.0010  ...   \n",
       "3                64.0         3.0         64.0         0.0010  ...   \n",
       "4                64.0         4.0         64.0         0.0010  ...   \n",
       "5                64.0         5.0         64.0         0.0010  ...   \n",
       "6                64.0         1.0         32.0         0.0010  ...   \n",
       "7                64.0         1.0         64.0         0.0010  ...   \n",
       "8                64.0         1.0        128.0         0.0010  ...   \n",
       "9                64.0         1.0        256.0         0.0010  ...   \n",
       "10               64.0         1.0         32.0         0.0001  ...   \n",
       "11               64.0         1.0         32.0         0.0010  ...   \n",
       "12               64.0         1.0         32.0         0.0100  ...   \n",
       "\n",
       "          regularizer_bias  training_time  test_loss  accuracy  precision_0  \\\n",
       "model_id                                                                      \n",
       "1                      NaN         0.3920   0.448702    80.418       96.942   \n",
       "2                      NaN         0.4258   0.447681    80.746       96.896   \n",
       "3                      NaN         0.3914   0.448122    80.392       96.890   \n",
       "4                      NaN         0.6952   0.445132    80.244       96.918   \n",
       "5                      NaN         0.6646   0.441334    80.548       96.916   \n",
       "6                      NaN         0.3496   0.445958    80.330       97.000   \n",
       "7                      NaN         0.3920   0.448702    80.418       96.942   \n",
       "8                      NaN         0.5840   0.450252    80.858       96.870   \n",
       "9                      NaN         0.7780   0.449549    80.918       96.862   \n",
       "10                     NaN         0.4128   0.474191    77.822       97.070   \n",
       "11                     NaN         0.3496   0.445958    80.330       97.000   \n",
       "12                     NaN         0.3934   0.447344    80.232       96.886   \n",
       "\n",
       "          precision_1  recall_0  recall_1    f1_0    f1_1  \n",
       "model_id                                                   \n",
       "1               6.764    82.252    33.024  88.988  11.218  \n",
       "2               6.620    82.648    31.628  89.198  10.936  \n",
       "3               6.524    82.272    31.860  88.976  10.820  \n",
       "4               6.592    82.090    32.556  88.884  10.954  \n",
       "5               6.684    82.414    32.326  89.068  11.062  \n",
       "6               6.936    82.108    34.418  88.930  11.538  \n",
       "7               6.764    82.252    33.024  88.988  11.218  \n",
       "8               6.540    82.792    30.928  89.274  10.784  \n",
       "9               6.506    82.866    30.696  89.312  10.726  \n",
       "10              6.692    79.360    38.140  87.324  11.382  \n",
       "11              6.936    82.108    34.418  88.930  11.538  \n",
       "12              6.462    82.108    31.862  88.880  10.734  \n",
       "\n",
       "[12 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn_layers     : ['1', 2, 3, 4, 5]\n",
      "dnn_neurons    : ['32', 64, 128, 256]\n",
      "learning_rate  : [0.0001, '0.001', 0.01]\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "model_update  = False\n",
    "\n",
    "params = initial_params.copy()\n",
    "MODELs = []\n",
    "METRICs = []\n",
    "METRIC_MEAN = []\n",
    "model_id = 1\n",
    "verbose = 0\n",
    "for param_name, space in search_space.items():\n",
    "    for point in space:\n",
    "        clear_output(wait=True)\n",
    "        display(METRIC_MEAN)\n",
    "        params[param_name] = point\n",
    "        for key, value in search_space.items():\n",
    "            print(f'{key:<14} : {value}')\n",
    "        \n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for name in protein_names:\n",
    "            data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "            ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "            \n",
    "            # get X dataset\n",
    "            x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "            x_features = list(x_onehot.columns)\n",
    "            \n",
    "            # get Y dataset\n",
    "            y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "            y_labels = list(y_onehot.columns)\n",
    "            \n",
    "            for idx in ST_idx:\n",
    "                window_x = np.array(get_window(x_onehot, idx, params['window_size']))\n",
    "                window_x = window_x.reshape(-1)\n",
    "                label_y  = np.array(y_onehot.iloc[idx])\n",
    "                \n",
    "                data_x.append(window_x)\n",
    "                data_y.append(label_y)\n",
    "                \n",
    "        data_x = np.array(data_x)\n",
    "        data_y = np.array(data_y)\n",
    "        model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params)  # I don't know why, but this row is helping producing the same training result of a neural network\n",
    "\n",
    "        print('data x shape:', data_x.shape)\n",
    "        print('data y shape:', data_y.shape)\n",
    "        print('class y counts:', data_y.sum(0))\n",
    "        print(f'class y ratio: {(data_y.sum(0)/len(data_y)).round(4)}')\n",
    "        \n",
    "        splitter = StratifiedShuffleSplit(n_splits = 1, test_size = test_size, random_state = SEED)\n",
    "        train_idx, test_idx = list(splitter.split(data_x, data_y))[0]\n",
    "        \n",
    "        train_x = data_x[train_idx]\n",
    "        train_y = data_y[train_idx]\n",
    "        \n",
    "        test_x = data_x[test_idx]\n",
    "        test_y = data_y[test_idx]\n",
    "        \n",
    "        train_x, test_x = data_scaling(train_x, test_x)\n",
    "        \n",
    "        splitter_kf = KFold(n_splits = 5)\n",
    "        for cv_idx, (train_idx_kf, test_idx_kf) in enumerate(splitter_kf.split(train_x, train_y)):\n",
    "            train_x_kf, train_y_kf = train_x[train_idx_kf], train_y[train_idx_kf]\n",
    "            test_x_kf, test_y_kf   = train_x[test_idx_kf],  train_y[test_idx_kf]\n",
    "            \n",
    "            train_x_kf, train_y_kf = upsample_data(train_x_kf, train_y_kf) # up-sample the training dataset\n",
    "            \n",
    "            model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params) \n",
    "            model_name = name_model(f'{model_type}_KFOLD', params)\n",
    "            \n",
    "            model_folder  = f'./models/{model_name}_{train_x.shape}_{test_x.shape}'\n",
    "            if not os.path.exists(model_folder):\n",
    "                os.makedirs(model_folder)\n",
    "            model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "            metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "            \n",
    "            \n",
    "            if not os.path.exists(model_path) or model_update:\n",
    "                time_start = time.time()\n",
    "                history = model.fit(train_x_kf, train_y_kf, verbose=verbose, \n",
    "                                    epochs = 10000, callbacks = callbacks,\n",
    "                                    validation_data = (test_x_kf, test_y_kf))\n",
    "                time_end = time.time()\n",
    "                training_time = round((time_end - time_start)/60, 3)\n",
    "                \n",
    "                model.save_weights(model_path)\n",
    "                \n",
    "                test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "                model_metrics = {\n",
    "                    'model_id' : model_id,\n",
    "                    'cv_idx'   : cv_idx,\n",
    "                    **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "                    'train_y'     : train_y.shape[-1],\n",
    "                    'test_size'   : test_x.shape[0],\n",
    "                    **params,\n",
    "                    'regularizer_input' : params['regularizer']['input'],\n",
    "                    'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "                    'regularizer_bias' : params['regularizer']['bias'],\n",
    "                    'training_time': training_time,\n",
    "                    'test_loss': test_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "                    **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "                    **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "                \n",
    "                model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "                model_metrics.to_csv(metric_path, index=False)\n",
    "                \n",
    "            else:\n",
    "                model.load_weights(model_path)\n",
    "                model_metrics = pd.read_csv(metric_path, header=0)\n",
    "                \n",
    "            print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "            \n",
    "            model_metrics['model_id'] = model_id\n",
    "            METRICs.append(model_metrics)\n",
    "            MODELs.append(model)\n",
    "        \n",
    "        METRIC_MEAN = pd.concat(METRICs).groupby('model_id').mean()\n",
    "        f1_best = METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0].f1_1\n",
    "        print(f'best f1 score: {f1_best}')\n",
    "        model_id += 1\n",
    "    if param_name in ['learning_rate']: # for float-type parameters\n",
    "        best_value = float(METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0][param_name])\n",
    "        params[param_name] = best_value\n",
    "        search_space[param_name][search_space[param_name].index(best_value)] = f\"{best_value}\"\n",
    "    else: # for int-type parameters\n",
    "        best_value = int(METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0][param_name])\n",
    "        params[param_name] = best_value\n",
    "        search_space[param_name][search_space[param_name].index(best_value)] = f\"{best_value}\"\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(METRIC_MEAN)\n",
    "for key, value in search_space.items():\n",
    "    print(f'{key:<14} : {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape:  (11528, 420)\n",
      "data y shape:  (11528, 2)\n",
      "train x shape: (9222, 420)\n",
      "test  x shape: (2306, 2)\n"
     ]
    }
   ],
   "source": [
    "print('data x shape: ', data_x.shape)\n",
    "print('data y shape: ', data_y.shape)\n",
    "print('train x shape:', train_x.shape)\n",
    "print('test  x shape:', test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal parameters\n",
      "rnn_layers     : 1\n",
      "rnn_neurons    : 64\n",
      "dnn_layers     : 1\n",
      "dnn_neurons    : 32\n",
      "activation     : softmax\n",
      "loss           : categorical_crossentropy\n",
      "metrics        : accuracy\n",
      "optimizer_type : Adam\n",
      "learning_rate  : 0.001\n",
      "regularizer    : {'input': None, 'hidden': None, 'bias': None}\n",
      "window_size    : 10\n"
     ]
    }
   ],
   "source": [
    "print('optimal parameters')\n",
    "for key, value in params.items():\n",
    "    print(f'{key:<14} : {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model's general performance through Monte-Carlo cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape: (11528, 420)\n",
      "data y shape: (11528, 2)\n",
      "class y counts: [11100   428]\n",
      "class y ratio: [0.9629 0.0371]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 11.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 12.590000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 14.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 13.889999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 11.940000000000001\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "model_update = False\n",
    "            \n",
    "MODELs = []\n",
    "METRICs = []\n",
    "METRIC_MEAN = []\n",
    "verbose = 0\n",
    "        \n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for name in protein_names:\n",
    "    data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "    ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "    \n",
    "    # get X dataset\n",
    "    x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "    x_features = list(x_onehot.columns)\n",
    "    \n",
    "    # get Y dataset\n",
    "    y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "    y_labels = list(y_onehot.columns)\n",
    "    \n",
    "    for idx in ST_idx:\n",
    "        window_x = np.array(get_window(x_onehot, idx, params['window_size']))\n",
    "        window_x = window_x.reshape(-1)\n",
    "        label_y  = np.array(y_onehot.iloc[idx])\n",
    "        \n",
    "        data_x.append(window_x)\n",
    "        data_y.append(label_y)\n",
    "        \n",
    "data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "\n",
    "print('data x shape:', data_x.shape)\n",
    "print('data y shape:', data_y.shape)\n",
    "print('class y counts:', data_y.sum(0))\n",
    "print(f'class y ratio: {(data_y.sum(0)/len(data_y)).round(4)}')\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits = 5, test_size = test_size, random_state = SEED)\n",
    "for cv_idx, (train_idx, test_idx) in enumerate(splitter.split(data_x, data_y)):\n",
    "    train_x, train_y = data_x[train_idx], data_y[train_idx]\n",
    "    test_x , test_y  = data_x[test_idx],  data_y[test_idx]\n",
    "    \n",
    "    train_x, train_y = upsample_data(train_x, train_y) # up-sample the training dataset\n",
    "    \n",
    "    train_x, test_x = data_scaling(train_x, test_x)\n",
    "    \n",
    "    model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params)\n",
    "    model_name = name_model(f'{model_type}', params)\n",
    "    \n",
    "    \n",
    "    model_folder  = f'./models/{model_name}_{train_x.shape}_{test_x.shape}'\n",
    "    if not os.path.exists(model_folder):\n",
    "        os.makedirs(model_folder)\n",
    "    model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "    metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(model_path) or model_update:\n",
    "        time_start = time.time()\n",
    "        history = model.fit(train_x, train_y, verbose=verbose, \n",
    "                            epochs = 10000, callbacks = callbacks,\n",
    "                            validation_split = test_size/(1-test_size))\n",
    "        time_end = time.time()\n",
    "        training_time = round((time_end - time_start)/60, 3)\n",
    "        \n",
    "        model.save_weights(model_path)\n",
    "        \n",
    "        test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "        model_metrics = {\n",
    "            'model_id' : model_id,\n",
    "            'cv_idx'   : cv_idx,\n",
    "            **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "            'train_y'     : train_y.shape[-1],\n",
    "            'test_size'   : test_x.shape[0],\n",
    "            **params,\n",
    "            'regularizer_input' : params['regularizer']['input'],\n",
    "            'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "            'regularizer_bias' : params['regularizer']['bias'],\n",
    "            'training_time': training_time,\n",
    "            'test_loss': test_loss,\n",
    "            'accuracy': accuracy,\n",
    "            **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "            **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "            **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "        model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "        model_metrics.to_csv(metric_path, index=False)\n",
    "        \n",
    "    else:\n",
    "        model.load_weights(model_path)\n",
    "        model_metrics = pd.read_csv(metric_path, header=0)\n",
    "        \n",
    "    print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "    \n",
    "    METRICs.append(model_metrics)\n",
    "    MODELs.append(model)\n",
    "\n",
    "METRICs = pd.concat(METRICs)\n",
    "METRIC_MEAN = METRICs.groupby('model_id').mean()\n",
    "METRIC_STD = METRICs.groupby('model_id').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>...</th>\n",
       "      <th>regularizer_bias</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>17760</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.526099</td>\n",
       "      <td>75.89</td>\n",
       "      <td>97.11</td>\n",
       "      <td>6.48</td>\n",
       "      <td>77.25</td>\n",
       "      <td>40.70</td>\n",
       "      <td>86.05</td>\n",
       "      <td>11.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>17760</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.588330</td>\n",
       "      <td>73.50</td>\n",
       "      <td>97.52</td>\n",
       "      <td>7.18</td>\n",
       "      <td>74.37</td>\n",
       "      <td>51.16</td>\n",
       "      <td>84.39</td>\n",
       "      <td>12.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>17760</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.232</td>\n",
       "      <td>0.511851</td>\n",
       "      <td>75.67</td>\n",
       "      <td>97.70</td>\n",
       "      <td>8.11</td>\n",
       "      <td>76.53</td>\n",
       "      <td>53.49</td>\n",
       "      <td>85.83</td>\n",
       "      <td>14.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>17760</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.546221</td>\n",
       "      <td>73.11</td>\n",
       "      <td>97.85</td>\n",
       "      <td>7.89</td>\n",
       "      <td>73.69</td>\n",
       "      <td>58.14</td>\n",
       "      <td>84.07</td>\n",
       "      <td>13.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>17760</td>\n",
       "      <td>420</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.500908</td>\n",
       "      <td>76.32</td>\n",
       "      <td>97.23</td>\n",
       "      <td>6.93</td>\n",
       "      <td>77.61</td>\n",
       "      <td>43.02</td>\n",
       "      <td>86.32</td>\n",
       "      <td>11.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "0        13       0    17760      420        2       2306           1   \n",
       "0        13       1    17760      420        2       2306           1   \n",
       "0        13       2    17760      420        2       2306           1   \n",
       "0        13       3    17760      420        2       2306           1   \n",
       "0        13       4    17760      420        2       2306           1   \n",
       "\n",
       "   rnn_neurons  dnn_layers  dnn_neurons  ...  regularizer_bias  training_time  \\\n",
       "0           64           1           32  ...              None          0.921   \n",
       "0           64           1           32  ...              None          0.858   \n",
       "0           64           1           32  ...              None          1.232   \n",
       "0           64           1           32  ...              None          0.646   \n",
       "0           64           1           32  ...              None          0.879   \n",
       "\n",
       "  test_loss accuracy precision_0  precision_1  recall_0  recall_1   f1_0  \\\n",
       "0  0.526099    75.89       97.11         6.48     77.25     40.70  86.05   \n",
       "0  0.588330    73.50       97.52         7.18     74.37     51.16  84.39   \n",
       "0  0.511851    75.67       97.70         8.11     76.53     53.49  85.83   \n",
       "0  0.546221    73.11       97.85         7.89     73.69     58.14  84.07   \n",
       "0  0.500908    76.32       97.23         6.93     77.61     43.02  86.32   \n",
       "\n",
       "    f1_1  \n",
       "0  11.18  \n",
       "0  12.59  \n",
       "0  14.09  \n",
       "0  13.89  \n",
       "0  11.94  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRICs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>window_size</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17760.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.9072</td>\n",
       "      <td>0.534682</td>\n",
       "      <td>74.898</td>\n",
       "      <td>97.482</td>\n",
       "      <td>7.318</td>\n",
       "      <td>75.89</td>\n",
       "      <td>49.302</td>\n",
       "      <td>85.332</td>\n",
       "      <td>12.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                             \n",
       "13           2.0  17760.0    420.0      2.0     2306.0         1.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  window_size  \\\n",
       "model_id                                                                     \n",
       "13               64.0         1.0         32.0          0.001         10.0   \n",
       "\n",
       "          training_time  test_loss  accuracy  precision_0  precision_1  \\\n",
       "model_id                                                                 \n",
       "13               0.9072   0.534682    74.898       97.482        7.318   \n",
       "\n",
       "          recall_0  recall_1    f1_0    f1_1  \n",
       "model_id                                      \n",
       "13           75.89    49.302  85.332  12.738  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRIC_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>window_size</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210432</td>\n",
       "      <td>0.034449</td>\n",
       "      <td>1.479314</td>\n",
       "      <td>0.310757</td>\n",
       "      <td>0.6757</td>\n",
       "      <td>1.758408</td>\n",
       "      <td>7.289713</td>\n",
       "      <td>1.027093</td>\n",
       "      <td>1.249108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                               \n",
       "13        1.581139      0.0      0.0      0.0        0.0         0.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  window_size  \\\n",
       "model_id                                                                     \n",
       "13                0.0         0.0          0.0            0.0          0.0   \n",
       "\n",
       "          training_time  test_loss  accuracy  precision_0  precision_1  \\\n",
       "model_id                                                                 \n",
       "13             0.210432   0.034449  1.479314     0.310757       0.6757   \n",
       "\n",
       "          recall_0  recall_1      f1_0      f1_1  \n",
       "model_id                                          \n",
       "13        1.758408  7.289713  1.027093  1.249108  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRIC_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# function ConnectButton(){\n",
    "#     console.log(\"Connect pushed\");\n",
    "#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
    "# }\n",
    "\n",
    "# setInterval(ConnectButton,60000);\n",
    "# '''\n",
    "\n",
    "# from google.colab import drive\n",
    "# from os import chdir\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# project_path = '/content/drive/MyDrive/Gproject/o-linked-site-prediction-feature-augment'\n",
    "# chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set seed for the reproducible result\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of initial proteins: 104\n",
      "['A2ABU4', 'A2AHJ4', 'A2AKB9', 'A2AQ25', 'E9Q1P8', 'E9Q5G3', 'O08537', 'O09061', 'O35303', 'O70263']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/integrated_features' # we will get names from the augmented proteins\n",
    "protein_names = [x.split('.')[0] for x in os.listdir(data_dir) if x.split('.')[1] == 'csv'] # get protein name list to be processed for building machine learning models\n",
    "print('the number of initial proteins:', len(protein_names))\n",
    "print(protein_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameter optimization by K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ml_models import *\n",
    "\n",
    "epochs = 1000\n",
    "from keras.callbacks import EarlyStopping\n",
    "patience = 30\n",
    "callbacks = [EarlyStopping(patience=patience, restore_best_weights=True, monitor='val_loss')]\n",
    "\n",
    "test_size = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters\n",
      "rnn_layers     : 1\n",
      "rnn_neurons    : 64\n",
      "dnn_layers     : 3\n",
      "dnn_neurons    : 64\n",
      "activation     : softmax\n",
      "loss           : categorical_crossentropy\n",
      "metrics        : accuracy\n",
      "optimizer_type : Adam\n",
      "learning_rate  : 0.001\n",
      "regularizer    : {'input': None, 'hidden': None, 'bias': None}\n",
      "window_size    : 10\n"
     ]
    }
   ],
   "source": [
    "initial_params = default_params.copy()\n",
    "initial_params.update({\n",
    "    'window_size'    : 10\n",
    "    })\n",
    "\n",
    "print('initial parameters')\n",
    "for key, value in initial_params.items():\n",
    "    print(f'{key:<14} : {value}')\n",
    "\n",
    "search_space = {\n",
    "    'dnn_layers'    : [1, 2, 3, 4, 5],\n",
    "    'dnn_neurons'   : [32, 64, 128, 256],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of augmented features: 13\n",
      "continuous features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'npa(-3,-1)', 1: 'ppo(-7,-5)', 2: 'n(S/T)', 3: 'flexibility', 4: 'p(1)'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features:\n",
      "{0: 'residue', 1: 'side_-1', 2: 'side_1', 3: 'side_2', 4: 'side_3', 5: 'side_4', 6: 'side_5', 7: 'ss', 8: 'ss_angle'}\n"
     ]
    }
   ],
   "source": [
    "model_type = 'MLP_UP_BASIC'\n",
    "\n",
    "basic_columns = dict(pd.read_csv('./data/basic_columns.csv', header=0).values.squeeze())\n",
    "print('# of augmented features:', len(basic_columns))\n",
    "\n",
    "# set continuous input features\n",
    "x_cts = [x for x in basic_columns.keys() if basic_columns.get(x) != 'object']\n",
    "print('continuous features:')\n",
    "display(dict(zip(range(len(x_cts)), x_cts)))\n",
    "\n",
    "# set categorical input features\n",
    "x_cat = ['residue'] + \\\n",
    "        [x for x in basic_columns.keys() if basic_columns.get(x) == 'object']\n",
    "print('categorical features:')\n",
    "print(dict(zip(range(len(x_cat)), x_cat)))\n",
    "\n",
    "# input features\n",
    "x_var = x_cts + x_cat\n",
    "\n",
    "# set continuos output feature\n",
    "y_cts = []\n",
    "\n",
    "# set categorical output feature\n",
    "y_cat = ['positivity']\n",
    "\n",
    "# output features\n",
    "y_var = y_cts + y_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build amino acid sequence dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training with K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>regularizer_bias</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3760</td>\n",
       "      <td>0.534175</td>\n",
       "      <td>73.156</td>\n",
       "      <td>97.286</td>\n",
       "      <td>6.518</td>\n",
       "      <td>74.188</td>\n",
       "      <td>46.510</td>\n",
       "      <td>84.170</td>\n",
       "      <td>11.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>0.533415</td>\n",
       "      <td>73.686</td>\n",
       "      <td>97.264</td>\n",
       "      <td>6.518</td>\n",
       "      <td>74.774</td>\n",
       "      <td>45.582</td>\n",
       "      <td>84.528</td>\n",
       "      <td>11.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3608</td>\n",
       "      <td>0.536670</td>\n",
       "      <td>73.208</td>\n",
       "      <td>97.312</td>\n",
       "      <td>6.572</td>\n",
       "      <td>74.228</td>\n",
       "      <td>46.976</td>\n",
       "      <td>84.202</td>\n",
       "      <td>11.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4836</td>\n",
       "      <td>0.540062</td>\n",
       "      <td>72.958</td>\n",
       "      <td>97.254</td>\n",
       "      <td>6.406</td>\n",
       "      <td>73.998</td>\n",
       "      <td>46.046</td>\n",
       "      <td>84.040</td>\n",
       "      <td>11.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4982</td>\n",
       "      <td>0.538522</td>\n",
       "      <td>73.252</td>\n",
       "      <td>97.444</td>\n",
       "      <td>6.954</td>\n",
       "      <td>74.162</td>\n",
       "      <td>49.764</td>\n",
       "      <td>84.214</td>\n",
       "      <td>12.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.541445</td>\n",
       "      <td>72.774</td>\n",
       "      <td>97.340</td>\n",
       "      <td>6.590</td>\n",
       "      <td>73.740</td>\n",
       "      <td>47.906</td>\n",
       "      <td>83.900</td>\n",
       "      <td>11.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4982</td>\n",
       "      <td>0.538522</td>\n",
       "      <td>73.252</td>\n",
       "      <td>97.444</td>\n",
       "      <td>6.954</td>\n",
       "      <td>74.162</td>\n",
       "      <td>49.764</td>\n",
       "      <td>84.214</td>\n",
       "      <td>12.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6702</td>\n",
       "      <td>0.539587</td>\n",
       "      <td>73.174</td>\n",
       "      <td>97.252</td>\n",
       "      <td>6.436</td>\n",
       "      <td>74.238</td>\n",
       "      <td>45.814</td>\n",
       "      <td>84.190</td>\n",
       "      <td>11.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.533426</td>\n",
       "      <td>74.302</td>\n",
       "      <td>97.324</td>\n",
       "      <td>6.820</td>\n",
       "      <td>75.380</td>\n",
       "      <td>46.512</td>\n",
       "      <td>84.950</td>\n",
       "      <td>11.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>0.551029</td>\n",
       "      <td>71.838</td>\n",
       "      <td>97.340</td>\n",
       "      <td>6.456</td>\n",
       "      <td>72.740</td>\n",
       "      <td>48.604</td>\n",
       "      <td>83.250</td>\n",
       "      <td>11.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4982</td>\n",
       "      <td>0.538522</td>\n",
       "      <td>73.252</td>\n",
       "      <td>97.444</td>\n",
       "      <td>6.954</td>\n",
       "      <td>74.162</td>\n",
       "      <td>49.764</td>\n",
       "      <td>84.214</td>\n",
       "      <td>12.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6126</td>\n",
       "      <td>0.459320</td>\n",
       "      <td>79.714</td>\n",
       "      <td>96.994</td>\n",
       "      <td>6.766</td>\n",
       "      <td>81.458</td>\n",
       "      <td>34.652</td>\n",
       "      <td>88.486</td>\n",
       "      <td>11.244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                             \n",
       "1            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "2            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "3            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "4            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "5            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "6            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "7            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "8            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "9            2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "10           2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "11           2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "12           2.0   9222.0     73.0      2.0     2306.0         1.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  ...  \\\n",
       "model_id                                                       ...   \n",
       "1                64.0         1.0         64.0         0.0010  ...   \n",
       "2                64.0         2.0         64.0         0.0010  ...   \n",
       "3                64.0         3.0         64.0         0.0010  ...   \n",
       "4                64.0         4.0         64.0         0.0010  ...   \n",
       "5                64.0         5.0         64.0         0.0010  ...   \n",
       "6                64.0         5.0         32.0         0.0010  ...   \n",
       "7                64.0         5.0         64.0         0.0010  ...   \n",
       "8                64.0         5.0        128.0         0.0010  ...   \n",
       "9                64.0         5.0        256.0         0.0010  ...   \n",
       "10               64.0         5.0         64.0         0.0001  ...   \n",
       "11               64.0         5.0         64.0         0.0010  ...   \n",
       "12               64.0         5.0         64.0         0.0100  ...   \n",
       "\n",
       "          regularizer_bias  training_time  test_loss  accuracy  precision_0  \\\n",
       "model_id                                                                      \n",
       "1                      NaN         0.3760   0.534175    73.156       97.286   \n",
       "2                      NaN         0.3550   0.533415    73.686       97.264   \n",
       "3                      NaN         0.3608   0.536670    73.208       97.312   \n",
       "4                      NaN         0.4836   0.540062    72.958       97.254   \n",
       "5                      NaN         0.4982   0.538522    73.252       97.444   \n",
       "6                      NaN         0.4292   0.541445    72.774       97.340   \n",
       "7                      NaN         0.4982   0.538522    73.252       97.444   \n",
       "8                      NaN         0.6702   0.539587    73.174       97.252   \n",
       "9                      NaN         0.9046   0.533426    74.302       97.324   \n",
       "10                     NaN         0.5350   0.551029    71.838       97.340   \n",
       "11                     NaN         0.4982   0.538522    73.252       97.444   \n",
       "12                     NaN         0.6126   0.459320    79.714       96.994   \n",
       "\n",
       "          precision_1  recall_0  recall_1    f1_0    f1_1  \n",
       "model_id                                                   \n",
       "1               6.518    74.188    46.510  84.170  11.428  \n",
       "2               6.518    74.774    45.582  84.528  11.392  \n",
       "3               6.572    74.228    46.976  84.202  11.522  \n",
       "4               6.406    73.998    46.046  84.040  11.242  \n",
       "5               6.954    74.162    49.764  84.214  12.200  \n",
       "6               6.590    73.740    47.906  83.900  11.584  \n",
       "7               6.954    74.162    49.764  84.214  12.200  \n",
       "8               6.436    74.238    45.814  84.190  11.278  \n",
       "9               6.820    75.380    46.512  84.950  11.894  \n",
       "10              6.456    72.740    48.604  83.250  11.392  \n",
       "11              6.954    74.162    49.764  84.214  12.200  \n",
       "12              6.766    81.458    34.652  88.486  11.244  \n",
       "\n",
       "[12 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn_layers     : [1, 2, 3, 4, '5']\n",
      "dnn_neurons    : [32, '64', 128, 256]\n",
      "learning_rate  : [0.0001, '0.001', 0.01]\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "model_update  = False\n",
    "\n",
    "params = initial_params.copy()\n",
    "MODELs = []\n",
    "METRICs = []\n",
    "METRIC_MEAN = []\n",
    "model_id = 1\n",
    "verbose = 0\n",
    "for param_name, space in search_space.items():\n",
    "    for point in space:\n",
    "        clear_output(wait=True)\n",
    "        display(METRIC_MEAN)\n",
    "        params[param_name] = point\n",
    "        for key, value in search_space.items():\n",
    "            print(f'{key:<14} : {value}')\n",
    "        \n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for name in protein_names:\n",
    "            data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "            ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "            \n",
    "            # get X dataset\n",
    "            x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "            x_features = list(x_onehot.columns)\n",
    "            \n",
    "            # get Y dataset\n",
    "            y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "            y_labels = list(y_onehot.columns)\n",
    "            \n",
    "            for idx in ST_idx:\n",
    "                window_x = x_onehot.iloc[idx]\n",
    "                label_y  = np.array(y_onehot.iloc[idx])\n",
    "                \n",
    "                data_x.append(window_x)\n",
    "                data_y.append(label_y)\n",
    "                \n",
    "        data_x = np.array(data_x)\n",
    "        data_y = np.array(data_y)\n",
    "        model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params) # I don't know why, but this row is helping producing the same training result of a neural network\n",
    "        \n",
    "        print('data x shape:', data_x.shape)\n",
    "        print('data y shape:', data_y.shape)\n",
    "        print('class y counts:', data_y.sum(0))\n",
    "        print(f'class y ratio: {(data_y.sum(0)/len(data_y)).round(4)}')\n",
    "        \n",
    "        splitter = StratifiedShuffleSplit(n_splits = 1, test_size = test_size, random_state = SEED)\n",
    "        train_idx, test_idx = list(splitter.split(data_x, data_y))[0]\n",
    "        \n",
    "        train_x = data_x[train_idx]\n",
    "        train_y = data_y[train_idx]\n",
    "        \n",
    "        test_x = data_x[test_idx]\n",
    "        test_y = data_y[test_idx]\n",
    "        \n",
    "        train_x, test_x = data_scaling(train_x, test_x)\n",
    "        \n",
    "        splitter_kf = KFold(n_splits = 5)\n",
    "        for cv_idx, (train_idx_kf, test_idx_kf) in enumerate(splitter_kf.split(train_x, train_y)):\n",
    "            train_x_kf, train_y_kf = train_x[train_idx_kf], train_y[train_idx_kf]\n",
    "            test_x_kf, test_y_kf   = train_x[test_idx_kf],  train_y[test_idx_kf]\n",
    "            \n",
    "            train_x_kf, train_y_kf = upsample_data(train_x_kf, train_y_kf) # up-sample the training dataset\n",
    "            \n",
    "            model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params) \n",
    "            model_name = name_model(f'{model_type}_KFOLD', params)\n",
    "            \n",
    "            model_folder  = f'./models/{model_name}_{train_x.shape}_{test_x.shape}'\n",
    "            if not os.path.exists(model_folder):\n",
    "                os.makedirs(model_folder)\n",
    "            model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "            metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "            \n",
    "            \n",
    "            if not os.path.exists(model_path) or model_update:\n",
    "                time_start = time.time()\n",
    "                history = model.fit(train_x_kf, train_y_kf, verbose=verbose, \n",
    "                                    epochs = 10000, callbacks = callbacks,\n",
    "                                    validation_data = (test_x_kf, test_y_kf))\n",
    "                time_end = time.time()\n",
    "                training_time = round((time_end - time_start)/60, 3)\n",
    "                \n",
    "                model.save_weights(model_path)\n",
    "                \n",
    "                test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "                model_metrics = {\n",
    "                    'model_id' : model_id,\n",
    "                    'cv_idx'   : cv_idx,\n",
    "                    **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "                    'train_y'     : train_y.shape[-1],\n",
    "                    'test_size'   : test_x.shape[0],\n",
    "                    **params,\n",
    "                    'regularizer_input' : params['regularizer']['input'],\n",
    "                    'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "                    'regularizer_bias' : params['regularizer']['bias'],\n",
    "                    'training_time': training_time,\n",
    "                    'test_loss': test_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "                    **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "                    **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "                \n",
    "                model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "                model_metrics.to_csv(metric_path, index=False)\n",
    "                \n",
    "            else:\n",
    "                model.load_weights(model_path)\n",
    "                model_metrics = pd.read_csv(metric_path, header=0)\n",
    "                \n",
    "            print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "            \n",
    "            model_metrics['model_id'] = model_id\n",
    "            METRICs.append(model_metrics)\n",
    "            MODELs.append(model)\n",
    "        \n",
    "        METRIC_MEAN = pd.concat(METRICs).groupby('model_id').mean()\n",
    "        f1_best = METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0].f1_1\n",
    "        print(f'best f1 score: {f1_best}')\n",
    "        model_id += 1\n",
    "    if param_name in ['learning_rate']: # for float-type parameters\n",
    "        best_value = float(METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0][param_name])\n",
    "        params[param_name] = best_value\n",
    "        search_space[param_name][search_space[param_name].index(best_value)] = f\"{best_value}\"\n",
    "    else: # for int-type parameters\n",
    "        best_value = int(METRIC_MEAN.sort_values('f1_1', ascending=False).iloc[0][param_name])\n",
    "        params[param_name] = best_value\n",
    "        search_space[param_name][search_space[param_name].index(best_value)] = f\"{best_value}\"\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(METRIC_MEAN)\n",
    "for key, value in search_space.items():\n",
    "    print(f'{key:<14} : {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape:  (11528, 73)\n",
      "data y shape:  (11528, 2)\n",
      "train x shape: (9222, 73)\n",
      "test  x shape: (2306, 2)\n"
     ]
    }
   ],
   "source": [
    "print('data x shape: ', data_x.shape)\n",
    "print('data y shape: ', data_y.shape)\n",
    "print('train x shape:', train_x.shape)\n",
    "print('test  x shape:', test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal parameters\n",
      "rnn_layers     : 1\n",
      "rnn_neurons    : 64\n",
      "dnn_layers     : 5\n",
      "dnn_neurons    : 64\n",
      "activation     : softmax\n",
      "loss           : categorical_crossentropy\n",
      "metrics        : accuracy\n",
      "optimizer_type : Adam\n",
      "learning_rate  : 0.001\n",
      "regularizer    : {'input': None, 'hidden': None, 'bias': None}\n",
      "window_size    : 10\n"
     ]
    }
   ],
   "source": [
    "print('optimal parameters')\n",
    "for key, value in params.items():\n",
    "    print(f'{key:<14} : {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model's general performance through Monte-Carlo cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape: (11528, 73)\n",
      "data y shape: (11528, 2)\n",
      "class y counts: [11100   428]\n",
      "class y ratio: [0.9629 0.0371]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 11.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 11.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 10.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 12.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:103: RuntimeWarning: invalid value encountered in divide\n",
      "  train_data_sc = (train_data - x_min) / (x_max - x_min)\n",
      "c:\\Users\\honsu\\OneDrive\\Github\\glycosylation\\o-linked-site-prediction-feature-augment\\functions.py:104: RuntimeWarning: invalid value encountered in divide\n",
      "  test_data_sc  = (test_data - x_min)  / (x_max - x_min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 12.4\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "model_update = False\n",
    "            \n",
    "MODELs = []\n",
    "METRICs = []\n",
    "METRIC_MEAN = []\n",
    "verbose = 0\n",
    "        \n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for name in protein_names:\n",
    "    data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "    ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "    \n",
    "    # get X dataset\n",
    "    x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "    x_features = list(x_onehot.columns)\n",
    "    \n",
    "    # get Y dataset\n",
    "    y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "    y_labels = list(y_onehot.columns)\n",
    "    \n",
    "    for idx in ST_idx:\n",
    "        window_x = x_onehot.iloc[idx]\n",
    "        label_y  = np.array(y_onehot.iloc[idx])\n",
    "        \n",
    "        data_x.append(window_x)\n",
    "        data_y.append(label_y)\n",
    "        \n",
    "data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "\n",
    "print('data x shape:', data_x.shape)\n",
    "print('data y shape:', data_y.shape)\n",
    "print('class y counts:', data_y.sum(0))\n",
    "print(f'class y ratio: {(data_y.sum(0)/len(data_y)).round(4)}')\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits = 5, test_size = test_size, random_state = SEED)\n",
    "for cv_idx, (train_idx, test_idx) in enumerate(splitter.split(data_x, data_y)):\n",
    "    train_x, train_y = data_x[train_idx], data_y[train_idx]\n",
    "    test_x , test_y  = data_x[test_idx],  data_y[test_idx]\n",
    "    \n",
    "    train_x, train_y = upsample_data(train_x, train_y) # up-sample the training dataset\n",
    "    \n",
    "    train_x, test_x = data_scaling(train_x, test_x)\n",
    "    \n",
    "    model = MLP_CLS(data_x.shape[-1], data_y.shape[-1], params)\n",
    "    model_name = name_model(f'{model_type}', params)\n",
    "    \n",
    "    \n",
    "    model_folder  = f'./models/{model_name}_{train_x.shape}_{test_x.shape}'\n",
    "    if not os.path.exists(model_folder):\n",
    "        os.makedirs(model_folder)\n",
    "    model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "    metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(model_path) or model_update:\n",
    "        time_start = time.time()\n",
    "        history = model.fit(train_x, train_y, verbose=verbose, \n",
    "                            epochs = 10000, callbacks = callbacks,\n",
    "                            validation_split = test_size/(1-test_size))\n",
    "        time_end = time.time()\n",
    "        training_time = round((time_end - time_start)/60, 3)\n",
    "        \n",
    "        model.save_weights(model_path)\n",
    "        \n",
    "        test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "        model_metrics = {\n",
    "            'model_id' : model_id,\n",
    "            'cv_idx'   : cv_idx,\n",
    "            **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "            'train_y'     : train_y.shape[-1],\n",
    "            'test_size'   : test_x.shape[0],\n",
    "            **params,\n",
    "            'regularizer_input' : params['regularizer']['input'],\n",
    "            'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "            'regularizer_bias' : params['regularizer']['bias'],\n",
    "            'training_time': training_time,\n",
    "            'test_loss': test_loss,\n",
    "            'accuracy': accuracy,\n",
    "            **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "            **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "            **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "        model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "        model_metrics.to_csv(metric_path, index=False)\n",
    "        \n",
    "    else:\n",
    "        model.load_weights(model_path)\n",
    "        model_metrics = pd.read_csv(metric_path, header=0)\n",
    "        \n",
    "    print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "    \n",
    "    METRICs.append(model_metrics)\n",
    "    MODELs.append(model)\n",
    "\n",
    "METRICs = pd.concat(METRICs)\n",
    "METRIC_MEAN = METRICs.groupby('model_id').mean()\n",
    "METRIC_STD = METRICs.groupby('model_id').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>...</th>\n",
       "      <th>regularizer_bias</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>17760</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.587849</td>\n",
       "      <td>69.17</td>\n",
       "      <td>97.42</td>\n",
       "      <td>6.29</td>\n",
       "      <td>69.82</td>\n",
       "      <td>52.33</td>\n",
       "      <td>81.34</td>\n",
       "      <td>11.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>17760</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.666035</td>\n",
       "      <td>60.62</td>\n",
       "      <td>97.95</td>\n",
       "      <td>6.18</td>\n",
       "      <td>60.36</td>\n",
       "      <td>67.44</td>\n",
       "      <td>74.69</td>\n",
       "      <td>11.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>17760</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.390</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>65.48</td>\n",
       "      <td>97.53</td>\n",
       "      <td>6.06</td>\n",
       "      <td>65.81</td>\n",
       "      <td>56.98</td>\n",
       "      <td>78.59</td>\n",
       "      <td>10.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>17760</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.161</td>\n",
       "      <td>0.620496</td>\n",
       "      <td>65.22</td>\n",
       "      <td>98.04</td>\n",
       "      <td>6.87</td>\n",
       "      <td>65.18</td>\n",
       "      <td>66.28</td>\n",
       "      <td>78.30</td>\n",
       "      <td>12.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>17760</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>2306</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.591603</td>\n",
       "      <td>68.13</td>\n",
       "      <td>97.81</td>\n",
       "      <td>6.91</td>\n",
       "      <td>68.42</td>\n",
       "      <td>60.47</td>\n",
       "      <td>80.52</td>\n",
       "      <td>12.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "0        13       0    17760       73        2       2306           1   \n",
       "0        13       1    17760       73        2       2306           1   \n",
       "0        13       2    17760       73        2       2306           1   \n",
       "0        13       3    17760       73        2       2306           1   \n",
       "0        13       4    17760       73        2       2306           1   \n",
       "\n",
       "   rnn_neurons  dnn_layers  dnn_neurons  ...  regularizer_bias  training_time  \\\n",
       "0           64           5           64  ...              None          0.552   \n",
       "0           64           5           64  ...              None          0.679   \n",
       "0           64           5           64  ...              None          1.390   \n",
       "0           64           5           64  ...              None          1.161   \n",
       "0           64           5           64  ...              None          0.864   \n",
       "\n",
       "  test_loss accuracy precision_0  precision_1  recall_0  recall_1   f1_0  \\\n",
       "0  0.587849    69.17       97.42         6.29     69.82     52.33  81.34   \n",
       "0  0.666035    60.62       97.95         6.18     60.36     67.44  74.69   \n",
       "0  0.613147    65.48       97.53         6.06     65.81     56.98  78.59   \n",
       "0  0.620496    65.22       98.04         6.87     65.18     66.28  78.30   \n",
       "0  0.591603    68.13       97.81         6.91     68.42     60.47  80.52   \n",
       "\n",
       "    f1_1  \n",
       "0  11.24  \n",
       "0  11.33  \n",
       "0  10.96  \n",
       "0  12.45  \n",
       "0  12.40  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRICs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>window_size</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17760.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.9292</td>\n",
       "      <td>0.615826</td>\n",
       "      <td>65.724</td>\n",
       "      <td>97.75</td>\n",
       "      <td>6.462</td>\n",
       "      <td>65.918</td>\n",
       "      <td>60.7</td>\n",
       "      <td>78.688</td>\n",
       "      <td>11.676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                             \n",
       "13           2.0  17760.0     73.0      2.0     2306.0         1.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  window_size  \\\n",
       "model_id                                                                     \n",
       "13               64.0         5.0         64.0          0.001         10.0   \n",
       "\n",
       "          training_time  test_loss  accuracy  precision_0  precision_1  \\\n",
       "model_id                                                                 \n",
       "13               0.9292   0.615826    65.724        97.75        6.462   \n",
       "\n",
       "          recall_0  recall_1    f1_0    f1_1  \n",
       "model_id                                      \n",
       "13          65.918      60.7  78.688  11.676  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRIC_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>window_size</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344673</td>\n",
       "      <td>0.031303</td>\n",
       "      <td>3.317684</td>\n",
       "      <td>0.266927</td>\n",
       "      <td>0.399337</td>\n",
       "      <td>3.637763</td>\n",
       "      <td>6.33467</td>\n",
       "      <td>2.575533</td>\n",
       "      <td>0.697445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cv_idx  train_0  train_1  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                               \n",
       "13        1.581139      0.0      0.0      0.0        0.0         0.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  learning_rate  window_size  \\\n",
       "model_id                                                                     \n",
       "13                0.0         0.0          0.0            0.0          0.0   \n",
       "\n",
       "          training_time  test_loss  accuracy  precision_0  precision_1  \\\n",
       "model_id                                                                 \n",
       "13             0.344673   0.031303  3.317684     0.266927     0.399337   \n",
       "\n",
       "          recall_0  recall_1      f1_0      f1_1  \n",
       "model_id                                          \n",
       "13        3.637763   6.33467  2.575533  0.697445  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRIC_STD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
